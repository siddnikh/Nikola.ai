<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nikola.ai: Your very own companion</title>
    <link rel="stylesheet" type="text/css" id="applicationStylesheet" href="master.css" />
    <script id="applicationScript" type="text/javascript" src="master.js"></script>
</head>

<body>
    <div id="Web_1920___1">
        <div id="Header">
            <img id="nikola" src="nikola.png" srcset="nikola.png 1x, nikola@2x.png 2x">
            <div id="NIKOLA_i">
                <span>NIKOLA</span>
            </div>
            <div id="Your_very_own_personal_intelli">
                <span>Your very own personal intelligent companion.</span>
            </div>
            <div id="Brought_as_a_part_of_the_thera">
                <span>Brought as a part of the theraBeats suite.</span>
            </div>
            <a target="_blank" href="../index.html">
                <img id="theraBeats_black" src="theraBeats_black.png" srcset="theraBeats_black.png 1x, theraBeats_black@2x.png 2x"></a>
        </div>
        <div id="Overview">
            <div id="OVERVIEW">
                <span>OVERVIEW</span>
            </div>
            <div id="Nikola_is_your_very_own__frien">
                <span>Nikola is your very own, friendly companion. Trained using advanced Machine Learning methods and Deep Learning methods, Nikola can reply efficiently to your statements, crack jokes with you, and help you have a cheerful and jolly time.<br/>The AI Therapist is designed for people suffering with mental health disorders like clinical depression, stress, anxiety, etc. The bot can also be used for aid in healing Narcissistic Personality Disorder. For any person who is low on confidence, and is looking for a person to talk to, Nikola is the one he can surely be with. It is in most manners, better for a person whoâ€™s on low on confidence in a very crucial manner to be talking to a computer chatbot rather than a human at first.</span>
            </div>
        </div>
        <div id="Technical_Specs">
            <div id="Technical_Specifications">
                <span>Technical Specifications</span>
            </div>
            <div id="MATTER_FOR_THE_WEBSITE__Techni">
                <span>MATTER FOR THE WEBSITE<br/><br/>Technical Content :<br/><br/>The first step is preparing the dataset:<br/>Dataset : Cornell Movie-Dialogs Corpus, which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.<br/>movie_conversations.txt contains list of the conversation IDs and movie_lines.text contains the text of assoicated with each conversation ID. For further information regarding the dataset, please check the README file in the zip file.<br/>Nikola.ai is planned to be a Neural Machine Translator chatbot trained on about 40GB of data from Reddit. The code for the Neural Machine chatbot is prepared. However, due to constraints like lack of computational power and low bandwidth, we have prepared a proof of concept using Transformers.<br/>Transformer, proposed in the paper Attention is All You Need, is a neural network architecture solely based on self-attention mechanism and is very parallelizable.<br/>LOAD AND PREPROCESS DATA<br/>MAX_SAMPLES=25000 and the maximum length of the sentence to be MAX_LENGTH=40.<br/>We preprocess our dataset in the following order:<br/><br/>Extract MAX_SAMPLES conversation pairs into list of questions and `answers.<br/>Preprocess each sentence by removing special characters in each sentence.<br/>Build tokenizer (map text to ID and ID to text) using TensorFlow Datasets SubwordTextEncoder.<br/>Tokenize each sentence and add START_TOKEN and END_TOKEN to indicate the start and end of each sentence.<br/>Filter out sentence that has more than MAX_LENGTH tokens.<br/>Pad tokenized sentences to MAX_LENGTH<br/><br/><br/>CREATE TF.DATA.DATASET<br/>We are going to use the tf.data.Dataset API to contruct our input pipline in order to utilize features like caching and prefetching to speed up the training process.<br/>The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.<br/>During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.<br/>As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.<br/>To prevent the model from peaking at the expected output the model uses a look-ahead mask.<br/>Target is divided into decoder_inputs which padded as an input to the decoder and cropped_targets for calculating our loss and accuracy.<br/> <br/><br/>Attention<br/>SCALED DOT PRODUCT ATTENTION<br/>The scaled dot-product attention function used by the transformer takes three inputs: Q (query), K (key), V (value).<br/> <br/><br/>MULTI-HEAD ATTENTION<br/>Multi-head attention contains four segments:<br/><br/>Linear layers and split into heads.<br/>Scaled dot-product attention.<br/>Concatenation of heads.<br/>Final linear layer.<br/>Transformer<br/>MASKING<br/>create_padding_mask and create_look_ahead are helper functions to creating masks to mask out padded tokens, we are going to use these helper functions as tf.keras.layers.Lambda layers.<br/>Mask all the pad tokens (value 0) in the batch to ensure the model does not treat padding as input.<br/>POSITIONAL ENCODING<br/>Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.<br/>The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence.<br/> <br/> <br/><br/>ENCODER LAYER<br/>SUBPARTS :<br/><br/>Multi-head attention (with padding mask)<br/>2 dense layers followed by dropout The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.<br/>ENCODER<br/>The Encoder consists of:<br/><br/>Input Embedding<br/>Positional Encoding<br/>num_layers encoder layers<br/>The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder.<br/><br/>DECODER LAYER<br/>Each decoder layer contains a few sublayers:<br/><br/>Masked multi-head attention (with look ahead mask and padding mask)<br/>Multi-head attention (with padding mask). value and key receive the encoder output as inputs. query receives the output from the masked multi-head attention sublayer.<br/>2 dense layers followed by dropout<br/>As query receives the output from decoder's first attention block, and key receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output.<br/> <br/><br/>DECODER<br/>The Decoder consists of:<br/><br/>Output Embedding<br/>Positional Encoding<br/>N decoder layers<br/>The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer.<br/> <br/><br/>TRANSFORMER<br/>Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.<br/> <br/><br/>Train model<br/>INITIALIZE MODEL<br/>To keep this example small and relatively fast, the values for num_layers, d_model, and units have been reduced. See the paper for all the other versions of the transformer.<br/> <br/><br/>LOSS FUNCTION<br/>Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.<br/>Now we will compile the model.<br/><br/>FIT MODEL<br/>Train our transformer by simply calling model.fit()<br/><br/>Evaluation & Prediction<br/>Note: The model used here has less capacity and trained on a subset of the full dataset, hence its performance can be further improved.<br/>The final step is testing.</span><br>
            </div>
        </div>
        <div id="Try">
            <div id="Try_the_Demo_Version_right_now">
                <span>Try the Demo Version right now!</span>
            </div>
            <div id="Click_HERE_to_try_out_the_demo">
                <span>Click</span><a href="https://colab.research.google.com/drive/1UDGQrXus0QQeCELXP2ej5_dAm6bV55J4"><span style="font-style:normal;font-weight:bold;"> HERE </span></a><span>to try out the demo version.</span>
            </div>
        </div>
        <div id="Footer">
            <svg class="Rectangle_2">
			<rect fill="rgba(0,0,0,1)" id="Rectangle_2" rx="0" ry="0" x="0" y="0" width="1920" height="311">
			</rect>
		</svg>
            <img id="theraBeats" src="theraBeats.png" srcset="theraBeats.png 1x, theraBeats@2x.png 2x">
            <div id="An_application_of_the_theraBea">
                <span>An application of the theraBeats.</span>
            </div>
        </div>
    </div>
</body>

</html>